{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import datetime\n",
    "import tensorflow_addons as tfa \n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_rijksdata\n",
    "NUM = 10000 # NUMBER OF IMAGES TO USE\n",
    "MIN_ARTWORK_NUM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, label_train, X_val, Y_val, label_val = read_rijksdata.load_data(img_folder='out_img',\n",
    "                                                                                  labels_file='labels.txt', \n",
    "                                                                                  names_file='names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM:\n",
    "    X = X_train[:NUM,:,:,:]\n",
    "    Y = Y_train[:NUM]\n",
    "else:\n",
    "    X = X_train\n",
    "    Y = Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv('names.txt',delimiter = '/t',header=None,engine='python')\n",
    "counts = pd.DataFrame(Y).value_counts()\n",
    "min_counts = counts[counts>=MIN_ARTWORK_NUM]\n",
    "min_idx = [index[0] for index in min_counts.index.tolist()]\n",
    "\n",
    "min_mask_idx = []\n",
    "for i, Y_ in enumerate(Y):\n",
    "    if np.isin(Y_,min_idx):\n",
    "        min_mask_idx.append(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.bar(x=list(range(len(min_idx))),height=list(counts),align='center',width=0.9)\n",
    "plt.xlabel('Artist Index')\n",
    "plt.ylabel('# of Artworks')\n",
    "plt.title('Artwork Number Distribution')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.axhline(10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X[min_mask_idx,:,:,:]\n",
    "Y_ = Y[min_mask_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = len(list(set(Y_)))\n",
    "print(classes)\n",
    "\n",
    "counts = pd.DataFrame(Y_).value_counts()\n",
    "print(min(counts))\n",
    "\n",
    "print(X_.shape)\n",
    "print(Y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = len(list(set(Y_)))\n",
    "Y_hot = tf.one_hot(indices=Y_,depth=classes)\n",
    "#Y_val = tf.one_hot(indices=Y_val,depth=classes)\n",
    "print('# of classes:',classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (56,56,3)\n",
    "enet_kwargs = {'include_top':False,\n",
    "               'weights':'imagenet',\n",
    "               'input_tensor':None,\n",
    "               'input_shape':input_shape,\n",
    "               'pooling':None,\n",
    "               'classes':1,\n",
    "               'classifier_activation':'softmax'}\n",
    "enet_base = tf.keras.applications.efficientnet.EfficientNetB7(**enet_kwargs)\n",
    "#enet_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet = tf.keras.models.Sequential()\n",
    "enet.add(enet_base)\n",
    "enet.add(tf.keras.layers.GlobalMaxPooling2D())\n",
    "#enet.add(tf.keras.layers.Dropout(rate=0.01))\n",
    "enet.add(tf.keras.layers.Dense(classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False,label_smoothing=0.0,name='categorical_crossentropy')\n",
    "\n",
    "# metrics\n",
    "\n",
    "TopKs = []\n",
    "for k in [1,5]:\n",
    "    TopK = tf.keras.metrics.TopKCategoricalAccuracy(k=k, name='top_{}'.format(k))\n",
    "    TopKs.append(TopK)\n",
    "metrics = [\"acc\"]\n",
    "metrics.extend(TopKs)\n",
    "\n",
    "f1 = tfa.metrics.F1Score(num_classes=classes, threshold=0.05)\n",
    "metrics.append(f1)\n",
    "\n",
    "# Optimizer\n",
    "# very average Adam settings\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# compile it all\n",
    "enet.compile(\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_hot.shape, type(Y_hot))\n",
    "print(X_.shape, type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = enet.fit(x=X_,y=Y_hot,validation_split=.20, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "checkpoint_time = '{date:%Y-%m-%d_%H-%M}'.format(date=datetime.datetime.now())\n",
    "save_file = './checkpoints/enet_{}'.format(checkpoint_time)\n",
    "print('Saving to:',save_file)\n",
    "enet.save_weights(save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = enet.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_true=Y, y_pred=y_pred, labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_train, y_pred)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
